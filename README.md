# DBT + Airflow + Snowflake Data Pipeline Project

This project is a personal practice project aimed at strengthening my skills and passion for becoming a Data Engineer.

## ðŸ”§ Tools & Technologies
- **Snowflake**: Cloud-based data warehouse used as the main data storage.
- **DBT (Data Build Tool)**: Used for data modeling, staging, and business-level transformations.
- **Apache Airflow**: Orchestrates the pipeline and automates scheduled DBT runs.
- **Astronomer**: Local development environment for Airflow.

## ðŸ“Œ Objective
To simulate a real-world data pipeline that:
1. Extracts sample data from Snowflake (using their TPCH dataset).
2. Performs staging and transformation using DBT.
3. Schedules and automates the pipeline with Airflow.

## ðŸ“‚ Project Structure

```bash
â”œâ”€â”€ dags/ # Airflow DAGs
â”‚ â”œâ”€â”€ dbt_dag.py # Main DAG for running DBT
â”‚ â”œâ”€â”€ exampledag.py # Example DAG
â”‚ â””â”€â”€ dbt/ # DBT project folder
â”‚ â””â”€â”€ data_pipeline/
â”‚ â”œâ”€â”€ models/ # Staging and marts models
â”‚ â”œâ”€â”€ macros/ # Custom DBT macros
â”‚ â”œâ”€â”€ seeds/ # Seed data (optional)
â”‚ â”œâ”€â”€ snapshots/ # Snapshots (optional)
â”‚ â”œâ”€â”€ packages.yml # DBT dependencies
â”‚ â””â”€â”€ dbt_project.yml # DBT config
â”œâ”€â”€ airflow_settings.yaml # Airflow environment settings
â”œâ”€â”€ Dockerfile # For building Docker image
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ packages.txt # System packages
â”œâ”€â”€ tests/ # DAG tests
â”‚ â””â”€â”€ dags/test_dag_example.py
â”œâ”€â”€ plugins/ # (Optional) Airflow plugins
â”œâ”€â”€ include/ # (Optional) Additional resources
â”œâ”€â”€ README.md # Project documentation
```


![DAG Graph](https://github.com/user-attachments/assets/cea9c15c-f460-49ea-ae89-e7f3e823bce8)

![Snowflake](https://github.com/user-attachments/assets/02edb6eb-0938-472a-b135-6f2b87d20aa7)

![Visual Studio Code](https://github.com/user-attachments/assets/0a5df1be-36b5-4ff1-9436-775b0d9cb7c0)
